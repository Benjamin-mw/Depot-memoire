---
title: "k-Means"
author: "Eudes Lapaz"
date: "2023-04-01"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

## Import Package

```{r}
library(FactoMineR)
library(factoextra)
library(CASdatasets)
library(tidyverse)
library(MASS)
library(knitr)
library(ggplot2)
library(cowplot)
library(reshape2)
library(dplyr)
library(GGally)
library(corrplot)
library(carData) 
library(car)
library(questionr)
library(multcomp)
library(dplyr)
library(leaps)
library(TeachingDemos)
library(FactoMineR)
library(factoextra)
library(ROCR)
library(plotROC)
```

## Etude CSP

Une première analyse nous permet de rassembler toutes les CSP comportant moins de 1,5% des assurés dans une seule et même classe. Il s'agit des classes : CSP17,2,20,21,22,26,3,30,37,38,40,41,44,45,47,49,51,56,57,59,6,61,63,65 et 7.

La suite de notre étude ne portera donc que sur les classes :CSP1, 42, 46, 48, 50, 55, 60 et 66

```{r}
#J'ai procédé d'une autre manière. Voir encadré suivant
#On conserve que les CSP1, 42, 46, 48, 50, 55, 60 et 66

#data=subset(freMPL5,freMPL5$SocioCateg %in% c("CSP1","CSP42","CSP46","CSP48","CSP50","CSP55","CSP60","CSP66"))

```

```{r}
#On importe la base de données qu'avec les bonnes CSP après sélection sur Excel
data(freMPL5)
freMPL5 <- subset(freMPL5, freMPL5$ClaimAmount >= 0)
write.csv(freMPL5, file="databrute_1.csv", row.names = FALSE)
setwd("C:/Users/enceu/OneDrive/Bureau/Git_hub_Mémoire/Depot-memoire")
data=read.csv("databrute.csv",header=TRUE,sep=";",fill=F)
```

### I) K-Means

L'objectif est de constituer différents groupes d'individus à partir de leurs caractéristiques. Ces regroupement seront faits grâce à des similitudes partagées par des mêmes individus (à partir de calcul de distance : on se servira essentiellement de la distance euclidienne dans cette étude)

Cette méthode sera utilisée pour deux variables quantitatives : Montant sinistre moyen par CSP, Fréquence de sinistres par CSP et Exposure moyen par CSP

### a) Etude des outliers

```{r}
boxplot(data$ClaimAmount[data$ClaimInd==1] ~ data$SocioCateg[data$ClaimInd==1], ylim=c(0,15000))
```

### b) Clustering

Dans la suite de cette section, on va regrouper, si possible, les 8 CSP restantes grâce à la méthode des k-means

```{r}
#On centre chaque colonne de notre dataframe data afin de pouvoir procéder aux k-means 
data_1=read.csv("k_means.csv",header=TRUE,sep=";",fill=F,row.names=1)
df=scale(data_1)
#Etude du nombre de clusterings que l'on va considérer grâce à différents critères

fviz_nbclust(data1,  FUNcluster = kmeans ,c("silhouette", "wss", "gap_stat"),k.max=7)

km.res <- kmeans(df, 2, nstart = 10)
km.res


fviz_cluster(km.res, data = df, centroids = 2, repel = TRUE, ellipse.type = "norm")
```

Ce premier clustering nous permet de constituer 2 clusters parmi ces 8 classes :

-   Cluster 1 : CSP1, 42, 46, 48 et 66

-   Cluster 2 : CSP 50, 55 et 60

## K-means (avec l'ensemble des données)

```{r}
data_2=read.csv("data_3.csv",header=TRUE,sep=";",fill=F)

#On crée une nouvelle colonnes pour pouvoir mettre en nom de colonnes les CSP bien qu'il y ait redondance (R veut que chaque nom de ligne soit différent)
data_2$col_1 <- paste(rownames(data_2), data_2$SocioCateg, sep = "_")
rownames(data_2)=data_2$col_1

#On donne une couleur à chaque CSP pour mieux les visualiser au final 
#data_2$couleur <- ifelse("CSP1" %in% row.names(my_data) , "red",ifelse("CSP42" %in% row.names(my_data),"blue",ifelse( "CSP46" %in% row.names(my_data),"green",ifelse("CSP48" %in% row.names(my_data),"yellow",ifelse("CSP50" %in% row.names(my_data),"black",ifelse("CSP55" %in% row.names(my_data),"orange",ifelse("CSP60" %in% row.names(my_data),"pink","purple")))))))

#On élimine les colonnes SocioCateg et col_1 qui n'ont plus d'utilité
data_2=data_2[, -c(1,11)]

#On prend n lignes de notre data set de manière aléatoire car il n'est pas possible de travailler avec les 25000 lignes (manque de puissance)

n=2000
data_2_aleatoire=sample_n(data_2, n)
```

### Clustering 

```{r}
#On centre chaque colonne de notre dataframe data afin de pouvoir procéder aux k-means 

df_1=scale(data_2_aleatoire)
#Etude du nombre de clusterings que l'on va considérer grâce à différents critères

fviz_nbclust(data_2_aleatoire,  FUNcluster = kmeans ,c("silhouette", "wss", "gap_stat"),k.max=7)

km.res <- kmeans(df_1, 2, nstart = 10)
km.res


fviz_cluster(km.res, data = df_1, centroids = 2, repel = TRUE, ellipse.type = "norm")
```

```{r}
# Chargement des packages
library(ggpubr)
library(cluster)

# Génération des données
set.seed(123)
data <- matrix(rnorm(200, mean = 5, sd = 2), ncol = 4)

# Calcul des clusters k-means
km <- kmeans(data, centers = 3)

# Conversion des données en data frame
plot.data <- data.frame(data)
plot.data$cluster <- as.factor(km$cluster)

# Affichage du graphique d'ellipse k-means
ggscatter(plot.data, x = "V1", y = "V2", ellipse.type = "convex", 
          fill = plot.data$cluster, palette = c("#00AFBB", "#E7B800", "#FC4E07"), 
          shape = 21, size = 3, legend.title = "Cluster")


```
